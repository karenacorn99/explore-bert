{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab Setup ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload /bert which is in my Desktop nlp folder\n",
    "# uploead bert base model /uncased_L-12_H-768_A-12 which is also in my Desktop nlp folder\n",
    "# upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Google Colab Mount Drive ###\n",
    "\n",
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Tensorflow version\n",
    "# Google default is 2.x, but it does not work with Bert pretraining\n",
    "# gives flag error\n",
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU, please give me Tesla P100 PCI-E 16 GB\n",
    "import torch\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd to where bert code is \n",
    "%cd drive/My\\ Drive/bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download transformer repo\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pytorch-pretrained-bert repo\n",
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertConfig, BertModel, BertForPreTraining, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_json_file('../uncased_L-12_H-768_A-12/bert_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg Word2Vec ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download GoogleNews-vectors-negative300.bin\n",
    "# should be in my Desktop nlp folder\n",
    "\n",
    "# load word2vec matrix\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../../../nlp/GoogleNews-vectors-negative300.bin', binary = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model.word_vec('social')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent each sentence as the average of the word2vec vector of\n",
    "# each individual word (with names removed)\n",
    "\n",
    "# preprocess a string, remove punctuation, convert to lowercase\n",
    "# text: string\n",
    "# return: preprocessed string\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return text.lower()\n",
    "\n",
    "# get the mean vector for a string\n",
    "# words: string\n",
    "def get_mean_vector(model, words):\n",
    "    names = ['amy', 'jenny', 'mitch', 'john', 'alice', 'sam', 'jeff', 'mark', 'kate', 'jane', 'naomi', 'noah', 'matthew', 'emma', 'neil', 'james', 'susan', 'olivia', 'jacob', 'tony']\n",
    "    # remove out-of-vocabulary words\n",
    "    words = words.split()\n",
    "    words = [word for word in words if word in model.vocab and word not in names]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean([model.word_vec(word) for word in words], axis = 0)\n",
    "        #return np.mean(wv_from_bin.word_vec(word), axis=0)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# get the cosine similarity of two mean vectors\n",
    "# context: avg word2vec vector for context sentence(s)\n",
    "# choice: avg word2vec vector for choice sentence(s)\n",
    "# return: float\n",
    "def get_similarity(context, choice):\n",
    "    return cosine_similarity(context.reshape(1, 300), choice.reshape(1, 300))\n",
    "\n",
    "# solve social mcq\n",
    "# model: word2vec model\n",
    "# test_folder: directory containing test file\n",
    "# return: list of pairwise similarity between context of choice\n",
    "# ie. every question takes 5 rows if there are 5 choices\n",
    "def word2vec_solver(model, test_folder):\n",
    "    test_file = test_folder + 'test.tsv'\n",
    "    df = pd.read_csv(test_file, sep = '\\t')\n",
    "    similarity = []\n",
    "    for _, row in df.iterrows():\n",
    "        sim = get_similarity(get_mean_vector(model, preprocess(row['#1 String'])), \n",
    "                             get_mean_vector(model, preprocess(row['#2 String'])))\n",
    "        similarity.append([sim[0][0], row['Quality']])\n",
    "    pd.DataFrame(similarity, columns = ['similarity', 'label']).to_csv(test_folder + 'similarity.csv', header = True, index = False)\n",
    "    return similarity\n",
    "\n",
    "# get accuracy\n",
    "def mcq_accuracy(similarity, num_choices):\n",
    "    #predictions = np.array(pd.read_csv(similarity_file, sep = '\\t')).reshape((-1, num_choices))\n",
    "    predictions = np.array(similarity)[:,0].reshape((-1, num_choices))\n",
    "    predicted_labels = np.argmax(predictions, axis = 1)\n",
    "    return np.sum(predicted_labels == 0) / 125\n",
    "\n",
    "# get recall@k\n",
    "# correct answer is predicted in top k\n",
    "from collections import Counter\n",
    "def top_k_correct(similarity, num_choices, topk):\n",
    "    #predictions = np.array(pd.read_csv(similarity_file, sep = '\\t')).reshape((-1, num_choices))\n",
    "    predictions = np.array(similarity)[:,0].reshape((-1, num_choices))\n",
    "    #predictions = np.array(similarity).reshape((-1, num_choices))\n",
    "    #predicted_labels = np.argmax(predictions, axis = 1)\n",
    "    # get index of correct label\n",
    "    #ranks = np.argsort(predictions, axis = 1)\n",
    "    # check whether the index of correct choies is in topk\n",
    "    indices = []\n",
    "    for p in predictions:\n",
    "        correct = p[0]\n",
    "        sorted_array = sorted(p, reverse = True)\n",
    "        indices.append(sorted_array.index(correct))\n",
    "    assert len(indices) == 125\n",
    "    #print(Counter(indices))\n",
    "    return sum(np.array(indices) < topk) / 125\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to use Word2VecSolver\n",
    "similarity = word2vec_solver(model, \"./goal_mcq_full/five_choices/fold10/\")\n",
    "print(mcq_accuracy)\n",
    "print(top_k_correct(similarity, 5, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Pretrained Bert Next Sent ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run below on google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Google Colab Mount Drive ###\n",
    "\n",
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/My\\ Drive/bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from transformers import BertConfig, BertModel, BertForNextSentencePrediction, BertForPreTraining, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "seq_A = 'I like cookies !'\n",
    "seq_B = 'Mitochondia are the powerhouse of the cell .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model and pretrained tokenizer\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits always changing\n",
    "# and wrong predictions\n",
    "config = modeling_bert.BertConfig.from_json_file(\"../social/uncased_L-12_H-768_A-12/bert_config.json\")\n",
    "model = BertForNextSentencePrediction(config)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct and stable\n",
    "config = BertConfig.from_json_file('../social/uncased_L-12_H-768_A-12/bert_config.json')\n",
    "model = BertForPreTraining.from_pretrained('../social/uncased_L-12_H-768_A-12/bert_model.ckpt.index', from_tf=True, config=config)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined as one input to the model\n",
    "encoded = tokenizer.encode_plus(seq_A, text_pair=seq_B, return_tensors='pt')\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_relationship_logits = model(**encoded)[0]\n",
    "print(seq_relationship_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert logits to probabilities \n",
    "# index 0: sequence B is a continuation of sequence A\n",
    "# index 1: sequence B is a random sequence\n",
    "probs = softmax(seq_relationship_logits, dim = 1)\n",
    "print(probs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label = np.argmax(probs.detach().numpy(), axis = 1)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve from files \n",
    "# ultimate bigthree: config, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse #1 String and #2 String for resolution prediction\n",
    "def bert_nextsent_solver(test_folder):\n",
    "    test_file = test_folder + \"test.tsv\"\n",
    "    df = pd.read_csv(test_file, sep = '\\t')\n",
    "    predictions = []\n",
    "    for _, row in df.iterrows():\n",
    "        seqA = row['#1 String']\n",
    "        seqB = row['#2 String']\n",
    "        encoded = tokenizer.encode_plus(seqA, text_pair=seqB, return_tensors='pt')\n",
    "        seq_relationship_logits = model(**encoded)[0]\n",
    "        probs = softmax(seq_relationship_logits, dim = 1).detach().numpy()\n",
    "        predicted_label = np.argmax(probs, axis = 1)\n",
    "        predictions.append([probs[0][0], predicted_label[0], 1 - row['Quality']])\n",
    "        pd.DataFrame(predictions, columns = ['similarity', 'predicted_label', 'label']).to_csv(test_folder + 'bert_pretrained.csv', header = True, index = False)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of binary classificaton\n",
    "def task_accuracy(predictions):\n",
    "    predicted_labels = np.array(predictions)[:,1]\n",
    "    gold_labels = np.array(predictions)[:, 2]\n",
    "    return np.sum(np.equal(predicted_labels, gold_labels)) / len(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy for mcq \n",
    "def mcq_accuracy(predictions, num_choices):\n",
    "    predictions = np.array(predictions)[:,0].reshape((-1, num_choices))\n",
    "    predicted_labels = np.argmax(predictions, axis = 1)\n",
    "    return np.sum(predicted_labels == 0) / 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "predictions = bert_nextsent_solver(\"./fold1/\")\n",
    "print(task_accuracy(predictions))\n",
    "print(mcq_accuracy(predictions, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Next Sent Trained Attention ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained the attention layer/encoder\n",
    "# code file: create_pretraining_data.py\n",
    "#            run_pretraining.py\n",
    "# data: two sets of text files, resolution + outlook/full story\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_pretraining_data.py \\\n",
    "  --input_file=../attention_cv/fold1.txt \\\n",
    "  --output_file=../fold1/tf_examples.tfrecord \\\n",
    "  --vocab_file=../uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "  --do_lower_case=True \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --masked_lm_prob=0.15 \\\n",
    "  --dupe_factor=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_pretraining.py \\\n",
    "  --input_file=../fold1/tf_examples.tfrecord \\\n",
    "  --output_dir=../fold1/pretraining_output \\\n",
    "  --do_train=True \\\n",
    "  --do_eval=True \\\n",
    "  --bert_config_file=../uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "  --init_checkpoint=../uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "  --train_batch_size=32 \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --num_train_steps=100\\\n",
    "  --num_warmup_steps=10 \\\n",
    "  --learning_rate=2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same set up as bert for next sent prediction no pretrain attention\n",
    "# except for model\n",
    "# initialize model using config and tf checkpoint(created in run_pretraining.py)\n",
    "config = BertConfig.from_json_file('../uncased_L-12_H-768_A-12/bert_config.json')\n",
    "model = BertForPreTraining.from_pretrained('../fold1/pretraining_output/bert_model.ckpt-100.index', from_tf=True, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertForMultipleChoice no Pretrained Attention ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code: run_swag.py (from pytorch-pretrained-bert)\n",
    "#    or run_multiple_choice.py (in the attention repo, not the Bert repo)\n",
    "# edit preprocessing, add eval on training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_social_output.py \\ # edited version for run_swag\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_lower_case \\\n",
    "--data_dir ../outlook_partial/fold1/ \\\n",
    "--bert_model bert-base-uncased \\\n",
    "--max_seq_length 128 \\\n",
    "--train_batch_size 10 \\\n",
    "--learning_rate 2e-6 \\\n",
    "--num_train_epochs 2.0 \\\n",
    "--output_dir ../fold1_lr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_social_output.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_lower_case \\\n",
    "--data_dir ../resolution_partial/fold1/ \\\n",
    "--bert_model bert-base-uncased \\\n",
    "--max_seq_length 128 \\\n",
    "--train_batch_size 5 \\\n",
    "--learning_rate 2e-6 \\\n",
    "--num_train_epochs 2.0 \\\n",
    "--seed 44 \\\n",
    "--output_dir ../fold1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate from front of context\n",
    "!python run_social_output_outlook_full.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_lower_case \\\n",
    "--data_dir ../outlook_full/fold1/ \\\n",
    "--bert_model bert-base-uncased \\\n",
    "--max_seq_length 256 \\\n",
    "--train_batch_size 5 \\\n",
    "--learning_rate 2e-6 \\\n",
    "--num_train_epochs 2.0 \\\n",
    "--output_dir ../fold1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff file preprocessing\n",
    "!python run_social_output_res_full.py \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_lower_case \\\n",
    "--data_dir ../resolution_full/fold1/ \\\n",
    "--bert_model bert-base-uncased \\\n",
    "--max_seq_length 128 \\\n",
    "--train_batch_size 5 \\\n",
    "--learning_rate 2e-6 \\\n",
    "--num_train_epochs 2.0 \\\n",
    "--output_dir ../fold1/\n",
    "# the data file for this has two columns for context\n",
    "# context1: seed+buildup+climax\n",
    "# context2: outlook\n",
    "# in preprocessing, the three sequences are truncated together\n",
    "# ie truncate the front of the longest sequence\n",
    "# which cannot be done in encode plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertForMultipleChoice Pretrained Attention ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain attention layer/enocde, train BertForMultipleChoice on top\n",
    "# of the custom attention layer\n",
    "# if directly load custom attention weights into BertForMultipleChoice\n",
    "# will result in error \"has no attribute bias\"\n",
    "# the walkaround is to dump the pretained attention weights as a pytorch model for BertForPreTraining ie general bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code: create_pretraining_data.py\n",
    "#       run_pretraining.py\n",
    "#       run_multiple_choice.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_pretraining_data.py \\\n",
    "  --input_file=../attention_cv_full/fold1.txt \\\n",
    "  --output_file=../fold1/tf_examples.tfrecord \\\n",
    "  --vocab_file=../uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "  --do_lower_case=True \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --masked_lm_prob=0.15 \\\n",
    "  --dupe_factor=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_pretraining.py \\\n",
    "  --input_file=../fold1/tf_examples.tfrecord \\\n",
    "  --output_dir=../fold1/pretraining_output \\\n",
    "  --do_train=True \\\n",
    "  --do_eval=True \\\n",
    "  --bert_config_file=../uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "  --init_checkpoint=../uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "  --train_batch_size=32 \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --num_train_steps=10 \\\n",
    "  --num_warmup_steps=10 \\\n",
    "  --learning_rate=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pretrained attention weights\n",
    "# Bert config only loads configuration, not the weights\n",
    "config = BertConfig.from_json_file('../uncased_L-12_H-768_A-12/bert_config.json')\n",
    "model = BertForPreTraining.from_pretrained('../fold1/pretraining_output/model.ckpt-10.index', from_tf=True, config=config)\n",
    "model.save_pretrained(\"../fold1/pretraining_output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./run_multiple_choice.py \\\n",
    "--model_type bert \\\n",
    "--task_name swag \\\n",
    "--model_name_or_path ../fold1/pretraining_output/ \\\n",
    "--config_name ../uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "--tokenizer_name bert-base-uncased \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_test \\\n",
    "--do_lower_case \\\n",
    "--data_dir ../outlook_full/fold1/ \\\n",
    "--learning_rate 5e-5 \\\n",
    "--num_train_epochs 2 \\\n",
    "--max_seq_length 128 \\\n",
    "--output_dir ../fold1/outlook_output/ \\\n",
    "--per_gpu_eval_batch_size=4 \\\n",
    "--per_gpu_train_batch_size=4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--overwrite_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
