{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Bert ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Google Colab Mount Drive ###\n",
    "\n",
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ./where_I_need_to_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use new version of tensorflow\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "upload bert model folder, for example, \n",
    "uncased_L-12_H-768_A-12\n",
    "which is bert-base-uncased\n",
    "The folder consists of:\n",
    "bert_config.json: \n",
    "    sepcifies the model structure\n",
    "    Loading the configuration file and using this file to \n",
    "    initialize a model does NOT load the model weights. It \n",
    "    only affects the model's configuration.\n",
    "bert_model.ckpt.data-00000-of-00001 \n",
    "bert_model.ckpt.index\n",
    "bert_model.ckpt.meta\n",
    "    Store the weights and biases of model.\n",
    "vocab.txt:\n",
    "    Bert's vocab file\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Next-sentence prediction with pretrained bert ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForNextSentencePrediction, BertConfig, BertModel, BertForPreTraining, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Three ways to load model and tokenizer'''\n",
    "'''#1'''\n",
    "# load pretrained model and pretrained tokenizer\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "'''#2: this one does not work and I don't know why '''\n",
    "# logits always changing\n",
    "# and wrong predictions\n",
    "config = modeling_bert.BertConfig.from_json_file(\"/uncased_L-12_H-768_A-12/bert_config.json\")\n",
    "model = BertForNextSentencePrediction(config)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "'''#3: can use customized tf checkpoints'''\n",
    "# correct and stable\n",
    "config = BertConfig.from_json_file('../uncased_L-12_H-768_A-12/bert_config.json')\n",
    "model = BertForPreTraining.from_pretrained('../uncased_L-12_H-768_A-12/bert_model.ckpt.index', from_tf=True, config=config)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: #3 is slower, preferably use save_pretrained('./dir_to_model')\n",
    "# TODO: learn save_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Sentence Prediction Examples #\n",
    "seq_A = 'I like cookies !'\n",
    "seq_B = 'Mitochondia are the powerhouse of the cell .'\n",
    "# combined as one input to the model\n",
    "encoded = tokenizer.encode_plus(seq_A, text_pair=seq_B, return_tensors='pt')\n",
    "print(encoded)\n",
    "# if the model is a BertForNextSentencePrediction, use [0]\n",
    "seq_relationship_logits = model(**encoded)[0]\n",
    "# if the model is is a BertForPreTraining, use[1]\n",
    "# because it outputs logits for both masked language model \n",
    "# and next sentence prediction\n",
    "seq_relationship_logits = model(**encoded)[1]\n",
    "print(seq_relationship_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert logits to probabilities \n",
    "# index 0: sequence B is a continuation of sequence A\n",
    "# index 1: sequence B is a random sequence\n",
    "probs = softmax(seq_relationship_logits, dim = 1)\n",
    "print(probs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label = np.argmax(probs.detach().numpy(), axis = 1)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Finetune attention weights ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://github.com/google-research/bert/blob/master/README.md#pre-training-with-bert\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_pretraining_data.py \\\n",
    "  --input_file=../training_text.txt \\\n",
    "  --output_file=../tmp/tf_examples.tfrecord \\\n",
    "  --vocab_file=../uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "  --do_lower_case=True \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --masked_lm_prob=0.15 \\\n",
    "  --random_seed=12345 \\\n",
    "  --dupe_factor=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_pretraining.py \\\n",
    "  --input_file=/tmp/tf_examples.tfrecord \\\n",
    "  --output_dir=/tmp/pretraining_output \\\n",
    "  --do_train=True \\\n",
    "  --do_eval=True \\\n",
    "  --bert_config_file=../uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "  --init_checkpoint=../uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "  --train_batch_size=32 \\\n",
    "  --max_seq_length=128 \\\n",
    "  --max_predictions_per_seq=20 \\\n",
    "  --num_train_steps=20 \\\n",
    "  --num_warmup_steps=10 \\\n",
    "  --learning_rate=2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random note: the if \".index\" is specified in the file name, \n",
    "# it will be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load finetuned attnetion weights for next-sentence-prediction ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model as in section 2 #3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finetuning Bert for Classification ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use customized attention weights by specifying \n",
    "# the corresponding tf checkpoints for\n",
    "# init_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_classifier.py \\\n",
    "  --task_name=MRPC \\\n",
    "  --do_train=true \\\n",
    "  --do_eval=true \\\n",
    "  --do_predict=true \\\n",
    "  --data_dir=../fold1 \\\n",
    "  --vocab_file=../uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "  --bert_config_file=../uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "  --init_checkpoint=../tmp/model.ckpt-10000 \\\n",
    "  --max_seq_length=128 \\\n",
    "  --train_batch_size=32 \\\n",
    "  --learning_rate=2e-7 \\\n",
    "  --num_train_epochs=2.0 \\\n",
    "  --output_dir=../fold1_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
